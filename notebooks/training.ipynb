{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oblivisheee/miniconda3/lib/python3.11/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (5.2.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "#Initialize dependencies\n",
    "import tensorflow as tf\n",
    "import os\n",
    "BASE_FOLDER = '/Users/oblivisheee/Documents/novelsdreamer-ru-t4m/'\n",
    "os.chdir(BASE_FOLDER)\n",
    "%store -r\n",
    "from modules.transformer_custom import Transformer\n",
    "from modules.regularization import RegularizedDenseLayer\n",
    "from modules.data_preprocess import DataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_dirs = {\n",
    "    'train_russian': os.path.join(BASE_FOLDER, 'dataset/train/russian'),\n",
    "    'train_english': os.path.join(BASE_FOLDER, 'dataset/train/english'),\n",
    "    'valid_russian': os.path.join(BASE_FOLDER, 'dataset/valid/russian'),\n",
    "    'valid_english': os.path.join(BASE_FOLDER, 'dataset/valid/english')\n",
    "}\n",
    "\n",
    "data_gen = DataGenerator(train_russian_dir=dataset_dirs['train_russian'], \n",
    "                         train_english_dir=dataset_dirs['train_english'], \n",
    "                         valid_russian_dir=dataset_dirs['valid_russian'], \n",
    "                         valid_english_dir=dataset_dirs['valid_english'])\n",
    "(train_russian_data, train_english_data, valid_russian_data, valid_english_data) = data_gen.generate()\n",
    "\n",
    "dataset_train = {\n",
    "    'input': tf.data.Dataset.from_tensor_slices(train_english_data),\n",
    "    'target': tf.data.Dataset.from_tensor_slices(train_russian_data),\n",
    "}\n",
    "dataset_valid = {\n",
    "    'input': tf.data.Dataset.from_tensor_slices(valid_english_data),\n",
    "    'target': tf.data.Dataset.from_tensor_slices(valid_russian_data)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the Transformer layer\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(8500, 512)\n",
    "regularized_layer = RegularizedDenseLayer(512)\n",
    "\n",
    "model = Transformer(num_layers=6, d_model=512, num_heads=8, dff=2048,\n",
    "                          input_vocab_size=8500, target_vocab_size=8500, pe_input=10000,\n",
    "                          pe_target=10000, rate=0.1, embedding=embedding_layer, regularized_layer=regularized_layer)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='auto')\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(name='Adam',learning_rate=0.001,\n",
    "                                     epsilon=1e-8, amsgrad=True,\n",
    "                                     beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy'), \n",
    "           tf.keras.metrics.MeanSquaredError(name='mean_squared_error'), \n",
    "           tf.keras.metrics.Precision(thresholds=0.5, name='precision'),]\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_object, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "model.fit_model(input_dataset=dataset_train['input'], \n",
    "                target_dataset=dataset_train['target'], \n",
    "                valid_input_dataset=dataset_valid['input'], \n",
    "                valid_target_dataset=dataset_valid['target'], \n",
    "                epochs=10,\n",
    "                session_name='novelsdreamer-test',\n",
    "                batch_size=64, gradient_accumulation_steps=5,\n",
    "                shuffle=True,\n",
    "                save_model_each_epoch=True,\n",
    "                model_name='novelsdreamer-ru-t4m')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
